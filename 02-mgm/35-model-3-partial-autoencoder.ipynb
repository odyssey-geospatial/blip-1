{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86b8916-bdd0-46e6-8195-8de0a4a06ab4",
   "metadata": {},
   "source": [
    "### VERGE: Vector-mode Regional Geospatial Encoding\n",
    "# VERGE model implementation\n",
    "\n",
    "\n",
    "Here we build and train a \"masked geospatial model\". \n",
    "This is a model in which each inpout is a set of encoded geospatial entities,\n",
    "consisting of a cooncatenation of a multi-point proximity encoding and a one-hot label vector.\n",
    "Modeling consists of masking the labels for a random selection of entities, \n",
    "passing the data through an encoder-based architecutre to predicte the labels of masked entities. \n",
    "The idea is that the encodings then capture information about the region.\n",
    "\n",
    "## Version 2: Partial Autoencoder\n",
    "\n",
    "I';m trying an approache here, in which the model is trained to predict\n",
    "a subset of its inpout features. I'm calling it a \"partial autoencoder\".\n",
    "The idea is:\n",
    "* Train an embedding on sets of features A and B, whose verature vectors are concatenated together.\n",
    "* After the embedding part, add a \"head\" that predicts A based on B.\n",
    "* If that works, then presumably the model understands something about the relationship between the two.\n",
    "\n",
    "## Summary\n",
    "\n",
    "After trying quite a few runs, this does not seem to be working at all. \n",
    "I am going to abandon this effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d7995-33a4-44b6-a248-4d97c86ce57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import arrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca7e3-ead1-4ddd-b908-e3ecf2d2aef2",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee168238-1ed3-44c8-af9b-c1907aadfab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dimension of the (square) AOIs. Set thi to match what was used\n",
    "# when the tiles were created.\n",
    "aoi_size = 1000\n",
    "\n",
    "# This is the resolution of the MPP encoding.\n",
    "resolution = 50\n",
    "geo_encoding_dim = 400\n",
    "\n",
    "# Fraction of cases to use for training.\n",
    "train_fraction = 0.7\n",
    "\n",
    "# Sample this fraction of entities per tile\n",
    "mask_fraction = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c1f54-443b-4820-ae36-b00ed6d575d1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216238e-a075-4a1d-81cf-3269d8c82fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of labels.\n",
    "fname = 'data/labels.csv'\n",
    "labels = pd.read_csv(fname)\n",
    "n_classes = len(labels)\n",
    "print('%d labels in this dataset' % n_classes)\n",
    "\n",
    "label_id_lookup = {\n",
    "    z['label']: z['id']\n",
    "    for z in labels.to_dict('records')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ae19e-e8c4-41a4-8a34-9000d20698e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of input data files. Each file consists of a list of encodings for \n",
    "# a number of square tiles in a particluar AOI.\n",
    "globstring = 'data/encodings/*'\n",
    "fnames = glob.glob(globstring)\n",
    "print('%d input files' % len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cdcfd-701e-44e5-9565-840775cf6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read some data.\n",
    "tile_data_list = []\n",
    "for fname in fnames[:5]:\n",
    "    print('reading', fname)\n",
    "    with open(fname, 'rb') as source:\n",
    "        tile_data_list += pickle.load(source)\n",
    "\n",
    "# Divide things into training and validation sets.\n",
    "train_tiles = []\n",
    "val_tiles = []\n",
    "for t in tile_data_list:\n",
    "    if np.random.random() < train_fraction:\n",
    "        train_tiles.append(t)\n",
    "    else:\n",
    "        val_tiles.append(t)\n",
    "\n",
    "print('%d training instances' % len(train_tiles))\n",
    "print('%d validation instances' % len(val_tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96f414-d6d7-470f-be2d-d4445acab2bf",
   "metadata": {},
   "source": [
    "## Modeling setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ef620-9a45-4215-8fdb-b9b35f3820dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class wraps a list of input tile data as a pytorch dataset.\n",
    "# An instance cinsists of (1) the original features, which are concatenations of\n",
    "# an MPP necoding and a one-hot class label; (2) MPP encodings\n",
    "# for a sample of the elements in the tile; (3) labels of the entities in (2). \n",
    "\n",
    "class VergeDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_list, n_classes, mask_fraction=0.15):\n",
    "        self.data = data_list\n",
    "        self.n_classes = n_classes\n",
    "        self.mask_fraction = mask_fraction\n",
    "        self.encoding_dim = data_list[0].shape[1] - self.n_classes\n",
    "        \n",
    "        # When accessing any item, we will also be sampling from its available classes.\n",
    "        # But this dataset has a big class imbalance, so we will sample according\n",
    "        # to inverse probability. Here we compute the probability distribution of classes.\n",
    "        self.class_prob = {z: 0.0 for z in range(self.n_classes)}\n",
    "        n = 0.0\n",
    "        for d in data_list:\n",
    "            true_labels_onehot = d[:, :self.n_classes]\n",
    "            true_labels = np.argmax(true_labels_onehot, axis=1)\n",
    "            for label in true_labels:\n",
    "                self.class_prob[label] += 1.0\n",
    "            n += len(true_labels)\n",
    "        for label in self.class_prob:\n",
    "            self.class_prob[label] /= n\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        features = self.data[idx]\n",
    "        encodings = features[:, self.n_classes:]\n",
    "        true_labels_onehot = features[:, :self.n_classes]\n",
    "        true_labels = np.argmax(true_labels_onehot, axis=1)\n",
    "        n_entities = features.shape[0]\n",
    "\n",
    "        # Select entities for which to assign masked labels.\n",
    "        weights = []\n",
    "        for label in true_labels:\n",
    "            prob = self.class_prob[label]\n",
    "            weights.append(1.0 / prob)\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / np.sum(weights)\n",
    "        sample_size = int(np.ceil(self.mask_fraction * n_entities))\n",
    "        sample_indices = np.random.choice(n_entities, size=sample_size, replace=True, p=weights) \n",
    "\n",
    "        # Get encodings for those samples.\n",
    "        sample_encodings = torch.tensor(encodings[sample_indices], dtype=torch.float32)\n",
    "\n",
    "        # Get true labels for those samples.\n",
    "        sample_labels = torch.tensor(true_labels[sample_indices], dtype=torch.long)\n",
    "                \n",
    "        return (features, sample_encodings, sample_labels)\n",
    "\n",
    "# dataset = VergeDataset(tile_data_list, n_classes, mask_fraction=mask_fraction)\n",
    "# features, sample_encodings, sample_labels = dataset[0]\n",
    "# print('features.shape', features.shape)\n",
    "# print('sample_encodings.shape', sample_encodings.shape)\n",
    "# print('sample_labels.shape', sample_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd86c9-551f-42f7-a303-c8f99af4c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that puts together a batch. The main thing we are handling here\n",
    "# is padding, which only needs to be applied to the input features. \n",
    "def collate_fn(batch):\n",
    "    \n",
    "    features, sample_encodings, sample_labels = zip(*batch)\n",
    "    \n",
    "    batch_size = len(features)\n",
    "    feature_dim = features[0].shape[1]\n",
    "    encoding_dim = sample_encodings[0].shape[1]\n",
    "    \n",
    "    max_feature_count = max(x.shape[0] for x in features)\n",
    "    max_sample_count = max(x.shape[0] for x in sample_encodings)\n",
    "\n",
    "    padded_features = torch.zeros(batch_size, max_feature_count, feature_dim)\n",
    "    feature_attention_mask = torch.zeros(batch_size, max_feature_count, dtype=torch.bool)\n",
    "\n",
    "    padded_sample_encodings = torch.zeros(batch_size, max_sample_count, encoding_dim)\n",
    "    padded_sample_labels = torch.full((batch_size, max_sample_count), -100, dtype=torch.long)  # -100 is the \"ignore\" value\n",
    "    sample_attention_mask = torch.zeros(batch_size, max_sample_count, dtype=torch.bool)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        feature_count = features[i].shape[0]\n",
    "        padded_features[i, :feature_count] = torch.tensor(features[i], dtype=torch.float32)\n",
    "        feature_attention_mask[i, :feature_count] = 1\n",
    "\n",
    "        sample_count = sample_encodings[i].shape[0]\n",
    "        padded_sample_encodings[i, :sample_count] = sample_encodings[i]\n",
    "        padded_sample_labels[i, :sample_count] = sample_labels[i]\n",
    "        sample_attention_mask[i, :sample_count] = 1\n",
    "\n",
    "\n",
    "    return (\n",
    "        padded_features, feature_attention_mask, \n",
    "        padded_sample_encodings, padded_sample_labels, sample_attention_mask\n",
    "    )\n",
    "\n",
    "# dataset = VergeDataset(train_tiles, n_classes)\n",
    "# batch = [dataset[k] for k in [5, 6, 7, 8]]\n",
    "# features, feature_attention_mask, sample_encodings, sample_labels, sample_attention_mask = collate_fn(batch)\n",
    "# print('features.shape', features.shape)\n",
    "# print('feature_attention_mask.shape', feature_attention_mask.shape)\n",
    "# print('sample_encodings.shape', sample_encodings.shape)\n",
    "# print('sample_labels.shape', sample_labels.shape)\n",
    "# print('sample_attention_mask.shape', sample_attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b685c-980f-42d5-8d7a-a195b3f8705e",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030a547-f3d6-43c0-b167-decde1b40080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_dim, embed_dim, head_count):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_count = head_count\n",
    "        \n",
    "        weights_r = torch.Tensor(feature_dim, embed_dim)\n",
    "        self.weights_r = nn.Parameter(weights_r)\n",
    "        torch.nn.init.normal_(self.weights_r, mean=0.0, std=1.0)\n",
    "\n",
    "        weights_h = torch.Tensor(feature_dim, head_count)\n",
    "        self.weights_h = nn.Parameter(weights_h)\n",
    "        torch.nn.init.normal_(self.weights_h, mean=0.0, std=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        matrix_r = torch.matmul(x, self.weights_r)\n",
    "        matrix_h = torch.matmul(x, self.weights_h)\n",
    "        matrix_h = torch.nn.functional.softmax(matrix_h, dim=1)\n",
    "        matrix_e = torch.matmul(torch.transpose(matrix_r, 1, 2), matrix_h)\n",
    "        return torch.transpose(matrix_e, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccc2ea-8be7-4ac9-9c8f-9718446723ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, region_embedding_count, region_embedding_dim, geo_encoding_dim, class_count):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        weights_g = torch.Tensor(geo_encoding_dim, region_embedding_dim)\n",
    "        self.weights_g = nn.Parameter(weights_g)\n",
    "        torch.nn.init.normal_(self.weights_g, mean=0.0, std=1.0)\n",
    "\n",
    "        weights_c = torch.Tensor(region_embedding_count, class_count)\n",
    "        self.weights_c = nn.Parameter(weights_c)\n",
    "        torch.nn.init.normal_(self.weights_c, mean=0.0, std=1.0)\n",
    "\n",
    "    def forward(self, region_embeddings, sample_encodings):\n",
    "        matrix_g = torch.matmul(sample_encodings, self.weights_g)\n",
    "        matrix_r = torch.transpose(region_embeddings, 1, 2)\n",
    "        matrix_x = torch.matmul(matrix_g, matrix_r)\n",
    "        matrix_c = torch.matmul(matrix_x, self.weights_c)\n",
    "\n",
    "        return torch.nn.functional.gelu(matrix_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075bfddc-5d69-4797-866d-a300b20fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeospatialTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        geo_encoding_dim, # the dimension of the geometric encodings\n",
    "        class_count=n_classes,\n",
    "        transformer_dim=128, \n",
    "        region_embedding_dim=64, \n",
    "        region_embedding_count=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.geo_encoding_dim = geo_encoding_dim\n",
    "        self.class_count = class_count\n",
    "        self.feature_dim = geo_encoding_dim + class_count\n",
    "        self.region_embedding_dim = region_embedding_dim\n",
    "        self.region_embedding_count = region_embedding_count\n",
    "        self.transformer_dim = transformer_dim\n",
    "\n",
    "        self.input_proj = nn.Linear(self.feature_dim, self.transformer_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=4 * transformer_dim,\n",
    "            dropout=0.1,\n",
    "            batch_first=True  \n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.collector = Collector(self.transformer_dim, self.region_embedding_dim, self.region_embedding_count)\n",
    "        self.classifier = ClassifierHead(\n",
    "            self.region_embedding_count, self.region_embedding_dim, \n",
    "            self.geo_encoding_dim, self.class_count\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, features, feature_attention_mask, sample_encodings, sample_attention_mask):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape [batch_size, n_entities, encoding_dim]\n",
    "        attention_mask: Tensor of shape [batch_size, n_entities], with 1 for valid, 0 for padding\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.input_proj(features)\n",
    "\n",
    "        # Transformer expects padding mask: True for PAD tokens\n",
    "        pad_mask = (feature_attention_mask == 0)\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "        region_embeddings = self.collector(x)\n",
    "        logits = self.classifier(region_embeddings, sample_encodings)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "    def embed(self, features, feature_attention_mask, sample_encodings, sample_attention_mask):\n",
    "        \"\"\"\n",
    "        Returns an embedding for the input features\n",
    "        \"\"\"\n",
    "        x = self.input_proj(features)\n",
    "\n",
    "        # Transformer expects padding mask: True for PAD tokens\n",
    "        pad_mask = (feature_attention_mask == 0)\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "        region_embeddings = self.collector(x)\n",
    "        return region_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ed6b2-9e4e-4776-9808-8261fed8b306",
   "metadata": {},
   "source": [
    "### For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedea23-2671-49d1-85a8-ec5c557a8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeospatialTransformer(\n",
    "    geo_encoding_dim = geo_encoding_dim, \n",
    "    class_count = n_classes,\n",
    "    transformer_dim = 32, \n",
    "    region_embedding_dim = 16, \n",
    "    region_embedding_count = 4,\n",
    ")\n",
    "\n",
    "n_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('%d trainable parameters in model' % n_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d31215-685a-42f0-a834-c29f2b2c2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VergeDataset(train_tiles, n_classes)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,            # Tune depending on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,   # Key for padding variable-length instances\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "features, feature_attention_mask, sample_encodings, sample_labels, sample_attention_mask = dataloader.__iter__().__next__()\n",
    "print('features.shape', features.shape)\n",
    "print('feature_attention_mask.shape', feature_attention_mask.shape)\n",
    "print('sample_encodings.shape', sample_encodings.shape)\n",
    "print('sample_labels.shape', sample_labels.shape)\n",
    "print('sample_attention_mask.shape', sample_attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b48f07-69c3-4763-bf8c-736627a7a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed(features, feature_attention_mask, sample_encodings, sample_attention_mask)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8252f-0393-47b2-b0a5-ec23fa192bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0347ad-a9fc-4bb0-8051-70f4996b9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = model.forward(features, feature_attention_mask, sample_encodings, sample_attention_mask)\n",
    "# logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa310216-b622-42e2-a551-322943108993",
   "metadata": {},
   "source": [
    "### Real training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe57ad-3566-4961-81dd-6a08e500ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeospatialTransformer(\n",
    "    geo_encoding_dim = geo_encoding_dim, \n",
    "    class_count = n_classes,\n",
    "    transformer_dim = 128, \n",
    "    region_embedding_dim = 64, \n",
    "    region_embedding_count = 8,\n",
    ")\n",
    "\n",
    "n_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('%d trainable parameters in model' % n_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116f8b4-2ad4-47ad-8c3d-7128e93456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = VergeDataset(train_tiles, n_classes, mask_fraction=mask_fraction)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,            # Tune depending on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a432a9-dee4-42c8-a068-61d31492436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for features, feature_attention_mask, sample_encodings, sample_labels, sample_attention_mask in dataloader:\n",
    "        features = features.to(device)\n",
    "        feature_attention_mask = feature_attention_mask.to(device)\n",
    "        sample_encodings = sample_encodings.to(device)\n",
    "        sample_labels = sample_labels.to(device)\n",
    "\n",
    "        logits = model(features, feature_attention_mask, sample_encodings, sample_attention_mask)  \n",
    "        loss = criterion(\n",
    "            logits.view(-1, n_classes),\n",
    "            sample_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    timestring = arrow.get().isoformat()\n",
    "    print(f\"{timestring}: epoch {epoch+1}, loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2246e-df72-48f6-98fa-8be691be1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.graph_objects import Scatter\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "trace = Scatter(\n",
    "    x=np.arange(len(losses)), y=losses, name='loss', \n",
    "    mode='markers+lines'\n",
    ")\n",
    "fig.append_trace(trace, 1, 1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beddf030-22cc-4af2-83dc-ff0afa350d78",
   "metadata": {},
   "source": [
    "## Performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d17495-c0c4-4be4-a31c-155087e6f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cue up the validation dataset.\n",
    "dataset = VergeDataset(train_tiles, n_classes)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9eb29-a495-4271-a153-48a87b1d4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "cases = []\n",
    "\n",
    "model.train()\n",
    "for features, feature_attention_mask, sample_encodings, sample_labels, sample_attention_mask in dataloader:\n",
    "\n",
    "    print(features.shape)\n",
    "    features = features.to(device)\n",
    "    feature_attention_mask = feature_attention_mask.to(device)\n",
    "    sample_encodings = sample_encodings.to(device)\n",
    "    sample_labels = sample_labels.to(device)\n",
    "\n",
    "    logits = model(features, feature_attention_mask, sample_encodings, sample_attention_mask)  \n",
    "    loss = criterion(\n",
    "        logits.view(-1, n_classes),\n",
    "        sample_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    batch_size = logits.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        case_logits = logits[i]\n",
    "        case_probs = torch.softmax(case_logits, axis=1)\n",
    "        case_labels = sample_labels[i]\n",
    "        entity_count = len(case_labels)\n",
    "        for k in range(entity_count):\n",
    "            if case_labels[k].item() > 0:\n",
    "                cases.append({\n",
    "                    'true_label': case_labels[k].item(),\n",
    "                    'probs': case_probs[k, :].detach().numpy()\n",
    "                })\n",
    "    if len(cases) >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f6704-f518-41c2-ba91-14bbd0a0f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class_count = max(d[\"true_label\"] for d in cases) + 1\n",
    "probs_by_class = defaultdict(list)\n",
    "\n",
    "for d in cases:\n",
    "    label = d[\"true_label\"]\n",
    "    probs = np.array(d[\"probs\"])\n",
    "    probs_by_class[label].append(probs)\n",
    "\n",
    "# For each true class, compute the mean probability vector\n",
    "mean_probs = []\n",
    "for t in range(class_count):\n",
    "    if probs_by_class[t]:\n",
    "        mean = np.stack(probs_by_class[t]).mean(axis=0)\n",
    "    else:\n",
    "        mean = np.zeros(class_count)  # if no samples for this class\n",
    "    mean_probs.append(mean)\n",
    "\n",
    "# Convert to 2D array: [true_class, predicted_class]\n",
    "matrix = np.stack(mean_probs)  # shape [C, C]\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(matrix, cmap='viridis', aspect='auto')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Avg Predicted Probability')\n",
    "ax.set_title(\"Mean Predicted Probabilities by True Class\")\n",
    "ax.set_xlabel(\"Predicted Class\")\n",
    "ax.set_ylabel(\"True Class\")\n",
    "ax.set_xticks(range(class_count))\n",
    "ax.set_yticks(range(class_count))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0218989-e285-45e7-b7b8-a985aed60af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = max(d[\"true_label\"] for d in cases) + 1\n",
    "print(class_count)\n",
    "cmat = np.zeros((class_count, class_count))\n",
    "\n",
    "for d in cases:\n",
    "    true_label = d[\"true_label\"]\n",
    "    pred_label = np.argmax(d[\"probs\"])\n",
    "    cmat[true_label, pred_label] += 1\n",
    "\n",
    "cmat = cmat.clip(0, 500)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "im = ax.imshow(cmat, cmap='viridis', aspect='auto')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Number Of Cases')\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted Class\")\n",
    "ax.set_ylabel(\"True Class\")\n",
    "ax.set_xticks(range(class_count))\n",
    "ax.set_yticks(range(class_count))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc5faa-7414-4422-b7db-6c6599cdaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_lookup = {\n",
    "    z['id']: z['label']\n",
    "    for z in labels.to_dict('records')\n",
    "}\n",
    "means = np.mean(matrix, axis=0)\n",
    "for i in range(len(means)):\n",
    "    print('%40s [%2d] : %6.1f' % (label_name_lookup[i], i, means[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b917da7-b65b-4235-98fc-9ff83cc1d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 4\n",
    "label = label_name_lookup[ix]\n",
    "probs = matrix[ix, :]\n",
    "for i in range(len(probs)):\n",
    "    print('true: %s | pred: %-40s [%2d] : prob: %6.2f' % (label, label_name_lookup[i], i, probs[i] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ffe6f-192a-4f32-a2d2-1fd5e7934b32",
   "metadata": {},
   "source": [
    "## Check out class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81772a6a-333b-46b7-965b-c41044ffe610",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VergeDataset(val_tiles, n_classes)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a488412-0cf3-48bb-a18e-cf78c39aed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts = {z: 0 for z in range(n_classes)}\n",
    "for features, feature_attention_mask, sample_encodings, sample_labels, sample_attention_mask in dataloader:\n",
    "    for s in torch.flatten(sample_labels):\n",
    "        s = s.item()\n",
    "        if s >= 0:\n",
    "            sample_counts[s] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10948e74-5654-4116-b0c8-f7d5ce03828d",
   "metadata": {},
   "outputs": [],
   "source": [
    " for s in sorted(sample_counts.keys()):\n",
    "     print('[%2d] %-40s : %6.0f' % (s, label_name_lookup[s], sample_counts[s])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9d832-b2b0-42d6-abed-73bb842c5c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
