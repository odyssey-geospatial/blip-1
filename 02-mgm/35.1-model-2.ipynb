{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86b8916-bdd0-46e6-8195-8de0a4a06ab4",
   "metadata": {
    "id": "e86b8916-bdd0-46e6-8195-8de0a4a06ab4"
   },
   "source": [
    "### VERGE: Vector-mode Regional Geospatial Encoding\n",
    "# Masked Geospatial Model Implementation\n",
    "\n",
    "Here we build and train a \"masked geospatial model\".\n",
    "This is a model in which each input is a set of encoded geospatial entities,\n",
    "consisting of a concatenation of a multi-point proximity encoding and a one-hot label vector.\n",
    "Modeling consists of masking the labels for a random selection of entities,\n",
    "passing the data through an encoder-based architecutre to predicte the labels of masked entities.\n",
    "The idea is that the encodings then capture information about the region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8M2z1JqEbum-",
   "metadata": {
    "id": "8M2z1JqEbum-"
   },
   "source": [
    "## Processing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SDCVXcwwbuNz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16967,
     "status": "ok",
     "timestamp": 1753562095789,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "SDCVXcwwbuNz",
    "outputId": "5b328b6c-84d3-409c-d649-394a5da24b7f"
   },
   "outputs": [],
   "source": [
    "# Google colab setup\n",
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# project_home = '/content/drive/MyDrive/Projects/verge'\n",
    "# os.chdir(project_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rexpq0-tAp12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7346,
     "status": "ok",
     "timestamp": 1753562103138,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "rexpq0-tAp12",
    "outputId": "426f97ae-388e-4d7a-bbb4-cf0a1f669114"
   },
   "outputs": [],
   "source": [
    "# !pip install geo_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7CvwllV6ATky",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753562103139,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "7CvwllV6ATky"
   },
   "outputs": [],
   "source": [
    "# Local processing setup\n",
    "project_home = '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cXRuWo0AWx0",
   "metadata": {
    "id": "0cXRuWo0AWx0"
   },
   "source": [
    "## Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad6c64-0b24-4315-ae3c-6b3b21f6175d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753562103140,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "78ad6c64-0b24-4315-ae3c-6b3b21f6175d"
   },
   "outputs": [],
   "source": [
    "# The name of the ROI to use.\n",
    "roi_name = 'ne-dev'\n",
    "\n",
    "# The name of the general-purpose data directory.\n",
    "data_home = '%s/data' % (project_home)\n",
    "\n",
    "# The name of the ROI-specific data directory.\n",
    "roi_home = '%s/data/%s' % (project_home, roi_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EJh-qkB0byEf",
   "metadata": {
    "id": "EJh-qkB0byEf"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d7995-33a4-44b6-a248-4d97c86ce57c",
   "metadata": {
    "executionInfo": {
     "elapsed": 7657,
     "status": "ok",
     "timestamp": 1753562110794,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "b54d7995-33a4-44b6-a248-4d97c86ce57c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import json\n",
    "from geo_encodings import MPPEncoder\n",
    "\n",
    "import sys\n",
    "sys.path.append(project_home)\n",
    "from utils.geo_transformer import VergeDataset, verge_collate_fn, GeospatialTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca7e3-ead1-4ddd-b908-e3ecf2d2aef2",
   "metadata": {
    "id": "445ca7e3-ead1-4ddd-b908-e3ecf2d2aef2"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee54dc-61bc-4044-bdb1-176d262ee420",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1753562111486,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "bcee54dc-61bc-4044-bdb1-176d262ee420",
    "outputId": "da99ddf9-096c-4a54-81e5-05b2cc78bb65"
   },
   "outputs": [],
   "source": [
    "# Read the ROI definition.\n",
    "fname = '%s/roi.json' % roi_home\n",
    "with open(fname) as source:\n",
    "    roi = json.load(source)\n",
    "\n",
    "tile_size = roi['tile_size']\n",
    "encoding_resolution = roi['encoding_resolution']\n",
    "\n",
    "# We need the dimension of the encoding.\n",
    "encoder = MPPEncoder(\n",
    "    region=[0, 0, tile_size, tile_size],\n",
    "    resolution=encoding_resolution,\n",
    "    center=True\n",
    ")\n",
    "geo_encoding_dim = len(encoder)\n",
    "print('%d elements in encodings' % geo_encoding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee168238-1ed3-44c8-af9b-c1907aadfab2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1753562111529,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "ee168238-1ed3-44c8-af9b-c1907aadfab2",
    "outputId": "315fef67-738b-4343-d824-ec17b6b32072"
   },
   "outputs": [],
   "source": [
    "# A unique identifier for this run. This will be a component of any\n",
    "# output file names.\n",
    "run_id = '005'\n",
    "\n",
    "# What type of device to train on.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using device', device)\n",
    "\n",
    "# Fraction of cases to use for training.\n",
    "train_fraction = 0.8\n",
    "\n",
    "# Number of epochs to run.\n",
    "epoch_count = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c1f54-443b-4820-ae36-b00ed6d575d1",
   "metadata": {
    "id": "ae4c1f54-443b-4820-ae36-b00ed6d575d1"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216238e-a075-4a1d-81cf-3269d8c82fb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1753562111972,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "d216238e-a075-4a1d-81cf-3269d8c82fb8",
    "outputId": "b7eaf292-16a9-4d60-af26-e4b30ae73434"
   },
   "outputs": [],
   "source": [
    "# Read the list of labels.\n",
    "fname = '%s/labels.csv' % data_home\n",
    "labels = pd.read_csv(fname)\n",
    "n_classes = len(labels)\n",
    "print('%d labels in this dataset' % n_classes)\n",
    "\n",
    "label_id_lookup = {\n",
    "    z['label']: z['id']\n",
    "    for z in labels.to_dict('records')\n",
    "}\n",
    "\n",
    "label_name_lookup = {\n",
    "    z['id']: z['label']\n",
    "    for z in labels.to_dict('records')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea121c2-9ddf-4180-abff-2f5c7c6a9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file that gives class probabilities.\n",
    "fname = '%s/class_info.csv' % roi_home\n",
    "class_info = pd.read_csv(fname)\n",
    "print('%d class info records' % len(class_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qCD2RJu73g26",
   "metadata": {
    "id": "qCD2RJu73g26"
   },
   "source": [
    "## Load data\n",
    "The data exist as NPZ files containing features and label vectors. \n",
    "Each is in a sub-folder for its AOI. \n",
    "We want to divide into train / validation splits according to the AOI, \n",
    "not the individual tile. This reduces autocorrelation effects that could\n",
    "bias performance assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53d850-0eca-4112-9c9a-c1661846aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of AOI folders.\n",
    "globstring = '%s/encodings/*' % roi_home\n",
    "aoi_dnames = glob.glob(globstring)\n",
    "\n",
    "# Loop over those, adding their files to either the train or val sets.\n",
    "train_fnames = []\n",
    "val_fnames = []\n",
    "split_records = []\n",
    "np.random.seed(5)\n",
    "for aoi_dname in aoi_dnames:\n",
    "    globstring = '%s/*.npz' % aoi_dname\n",
    "    tile_fnames = glob.glob(globstring)\n",
    "    if np.random.random() < train_fraction:\n",
    "        split = 'train'\n",
    "        train_fnames += tile_fnames\n",
    "    else:\n",
    "        split = 'val'\n",
    "        val_fnames += tile_fnames\n",
    "    # print('added %d files to the %s set' % (len(tile_fnames), split))\n",
    "    split_records.append({'aoi': aoi_dname, 'split': split})\n",
    "    \n",
    "print('%d training instances' % len(train_fnames))\n",
    "print('%d validation instances' % len(val_fnames))\n",
    "\n",
    "# Save the split records\n",
    "fname = '%s/splits-%s.csv' % (roi_home, run_id)\n",
    "pd.DataFrame(split_records).to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd86c9-551f-42f7-a303-c8f99af4c9e1",
   "metadata": {
    "id": "11cd86c9-551f-42f7-a303-c8f99af4c9e1"
   },
   "outputs": [],
   "source": [
    "# # Test that.\n",
    "# dataset = VergeDataset(train_tiles, n_classes, mask_fraction=0.15)\n",
    "# batch = [dataset[k] for k in [0, 12, 17, 23]]\n",
    "# batch_features, batch_labels, batch_attention_mask = verge_collate_fn(batch)\n",
    "# print('test:')\n",
    "# print('batch_features.shape', batch_features.shape)\n",
    "# print('batch_labels.shape', batch_labels.shape)\n",
    "# print('batch_attention_mask.shape', batch_attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a992dc4-cdbb-49d5-a6cd-707a59389e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59162f11-1eca-4998-8cf0-0371d308bea5",
   "metadata": {
    "id": "59162f11-1eca-4998-8cf0-0371d308bea5"
   },
   "outputs": [],
   "source": [
    "# The dataset constructor requires a lookup table for class probabilities. \n",
    "class_prob_lookup = {\n",
    "    z['label']: z['prob']\n",
    "    for z in class_info.to_dict('records')\n",
    "}\n",
    "\n",
    "# Initialize training and validation datasets.\n",
    "train_dataset = VergeDataset(train_fnames, n_classes, mask_fraction=0.15, class_prob=class_prob_lookup)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8, # Tune depending on GPU memory\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=verge_collate_fn,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_dataset = VergeDataset(val_fnames, n_classes, mask_fraction=0.15, class_prob=class_prob_lookup)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8, # Tune depending on GPU memory\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=verge_collate_fn,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b685c-980f-42d5-8d7a-a195b3f8705e",
   "metadata": {
    "id": "419b685c-980f-42d5-8d7a-a195b3f8705e"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedea23-2671-49d1-85a8-ec5c557a8ea2",
   "metadata": {
    "id": "daedea23-2671-49d1-85a8-ec5c557a8ea2"
   },
   "outputs": [],
   "source": [
    "model = GeospatialTransformer(\n",
    "    feature_dim = geo_encoding_dim + n_classes,\n",
    "    model_dim=128,\n",
    "    num_heads=4,\n",
    "    num_layers=5,\n",
    "    num_classes=n_classes,\n",
    "    dropout=0.2\n",
    ")\n",
    "n_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('%d trainable parameters in model' % n_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d31215-685a-42f0-a834-c29f2b2c2b83",
   "metadata": {
    "id": "82d31215-685a-42f0-a834-c29f2b2c2b83"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "# dataset = VergeDataset(train_tiles, n_classes, mask_fraction=0.15)\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=2,            # Tune depending on GPU memory\n",
    "#     shuffle=True,\n",
    "#     collate_fn=verge_collate_fn,   # Key for padding variable-length instances\n",
    "#     drop_last=False\n",
    "# )\n",
    "\n",
    "# features, labels, attention_mask = dataloader.__iter__().__next__()\n",
    "# print(features.shape, labels.shape, attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cKIPZbfp3",
   "metadata": {
    "id": "927cKIPZbfp3"
   },
   "outputs": [],
   "source": [
    "# model(features, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa310216-b622-42e2-a551-322943108993",
   "metadata": {
    "id": "fa310216-b622-42e2-a551-322943108993"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a432a9-dee4-42c8-a068-61d31492436a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "80a432a9-dee4-42c8-a068-61d31492436a"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    # Training.\n",
    "    model.train()\n",
    "    for features, labels, attention_mask in train_dataloader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        logits = model(features, attention_mask)\n",
    "        loss = criterion(\n",
    "            logits.view(-1, n_classes),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, labels, attention_mask in val_dataloader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            logits = model(features, attention_mask)\n",
    "            val_loss = criterion(\n",
    "                logits.view(-1, n_classes),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "    losses.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': loss.item(),\n",
    "        'val_loss': val_loss.item()\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, train loss: {loss.item():.4f}, val_loss: {val_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L253y-1viYNM",
   "metadata": {
    "id": "L253y-1viYNM"
   },
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "model_fname = '%s/model-%s' % (roi_home, run_id)\n",
    "torch.save(model, model_fname)\n",
    "print('saved %s' % model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a93715-1774-4bc4-8813-7cddbb14fc96",
   "metadata": {
    "id": "08a93715-1774-4bc4-8813-7cddbb14fc96"
   },
   "source": [
    "## Loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2246e-df72-48f6-98fa-8be691be1242",
   "metadata": {
    "id": "c5d2246e-df72-48f6-98fa-8be691be1242"
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.graph_objects import Scatter\n",
    "\n",
    "epochs = [d['epoch'] for d in losses]\n",
    "train_losses = [d['train_loss'] for d in losses]\n",
    "val_losses = [d['val_loss'] for d in losses]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "trace = Scatter(\n",
    "    x=epochs, y=train_losses, name='training loss',\n",
    "    mode='markers+lines', marker_color='blue'\n",
    ")\n",
    "fig.append_trace(trace, 1, 1)\n",
    "\n",
    "trace = Scatter(\n",
    "    x=epochs, y=val_losses, name='validation loss',\n",
    "    mode='markers+lines', marker_color='green'\n",
    ")\n",
    "fig.append_trace(trace, 1, 1)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10079b1",
   "metadata": {
    "id": "c10079b1"
   },
   "source": [
    "## Validation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb80ef4",
   "metadata": {
    "id": "9fb80ef4"
   },
   "outputs": [],
   "source": [
    "# Process the validation dataset, getting the class probability predictions\n",
    "# for every instance.\n",
    "model.to(device)\n",
    "cases = []\n",
    "model.eval()\n",
    "for features, labels, attention_mask in val_dataloader:\n",
    "\n",
    "    features = features.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    logits = model(features, attention_mask)\n",
    "\n",
    "    batch_size = logits.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        case_logits = logits[i]\n",
    "        case_probs = torch.softmax(case_logits, dim=1)\n",
    "        case_labels = labels[i]\n",
    "        entity_count = len(case_labels)\n",
    "        for k in range(entity_count):\n",
    "            if case_labels[k].item() >= 0: # Skips the \"-100\" labels.\n",
    "                cases.append({\n",
    "                    'true_label': case_labels[k].item(),\n",
    "                    'probs': torch.Tensor.cpu(case_probs[k, :]).detach().numpy()\n",
    "                })\n",
    "\n",
    "print('compiled prediction probabilities for %d validation instances' % len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2752ef",
   "metadata": {
    "id": "dd2752ef"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class_count = max(d[\"true_label\"] for d in cases) + 1\n",
    "probs_by_class = defaultdict(list)\n",
    "\n",
    "for d in cases:\n",
    "    label = d[\"true_label\"]\n",
    "    probs = np.array(d[\"probs\"])\n",
    "    probs_by_class[label].append(probs)\n",
    "\n",
    "# For each true class, compute the mean probability vector\n",
    "mean_probs = []\n",
    "for t in range(class_count):\n",
    "    if probs_by_class[t]:\n",
    "        mean = np.stack(probs_by_class[t]).mean(axis=0)\n",
    "    else:\n",
    "        mean = np.zeros(class_count)  # if no samples for this class\n",
    "    mean_probs.append(mean)\n",
    "\n",
    "# Convert to 2D array: [true_class, predicted_class]\n",
    "matrix = np.stack(mean_probs)  # shape [C, C]\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "im = ax.imshow(matrix, cmap='viridis', aspect='auto')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Avg Predicted Probability')\n",
    "ax.set_title(\"Mean Predicted Probabilities by True Class\")\n",
    "ax.set_xlabel(\"Predicted Class\")\n",
    "ax.set_ylabel(\"True Class\")\n",
    "ax.set_xticks(range(class_count))\n",
    "ax.set_yticks(range(class_count))\n",
    "ax.set_yticklabels([label_name_lookup[i] for i in range(class_count)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25618be6",
   "metadata": {
    "id": "25618be6"
   },
   "outputs": [],
   "source": [
    "# class_count = max(d[\"true_label\"] for d in cases) + 1\n",
    "# print(class_count)\n",
    "# cmat = np.zeros((class_count, class_count))\n",
    "\n",
    "# for d in cases:\n",
    "#     true_label = d[\"true_label\"]\n",
    "#     pred_label = np.argmax(d[\"probs\"])\n",
    "#     cmat[true_label, pred_label] += 1\n",
    "\n",
    "# cmat = np.sqrt(cmat)\n",
    "\n",
    "# # Plot heatmap\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# im = ax.imshow(cmat, cmap='viridis', aspect='auto')\n",
    "\n",
    "# plt.colorbar(im, ax=ax, label='Number Of Cases')\n",
    "# ax.set_title(\"Confusion Matrix\")\n",
    "# ax.set_xlabel(\"Predicted Class\")\n",
    "# ax.set_ylabel(\"True Class\")\n",
    "# ax.set_xticks(range(class_count))\n",
    "# ax.set_yticks(range(class_count))\n",
    "# ax.set_yticklabels(['%s [%d]' % (label_name_lookup[i], i) for i in range(class_count)])\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc18b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 83177,
     "status": "aborted",
     "timestamp": 1753562161888,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "7acc18b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fd353-0e6e-42e7-aaf1-49e46d3f1720",
   "metadata": {
    "executionInfo": {
     "elapsed": 83179,
     "status": "aborted",
     "timestamp": 1753562161890,
     "user": {
      "displayName": "John Collins",
      "userId": "16643596247369517939"
     },
     "user_tz": 240
    },
    "id": "899fd353-0e6e-42e7-aaf1-49e46d3f1720"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
