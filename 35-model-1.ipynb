{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86b8916-bdd0-46e6-8195-8de0a4a06ab4",
   "metadata": {},
   "source": [
    "### VERGE: Vector-mode Regional Geospatial Encoding\n",
    "# VERGE model implementation\n",
    "\n",
    "\n",
    "Here we build and train a \"masked geospatial model\". \n",
    "This is a model in which each inpout is a set of encoded geospatial entities,\n",
    "consisting of a cooncatenation of a multi-point proximity encoding and a one-hot label vector.\n",
    "Modeling consists of masking the labels for a random selection of entities, \n",
    "passing the data through an encoder-based architecutre to predicte the labels of masked entities. \n",
    "The idea is that the encodings then capture information about the region.\n",
    "\n",
    "## Version 1:\n",
    "THis version uses a basic architecture proposed by ChatGPT. Of course I have modified \n",
    "the original suggestion heavily. This helped to establish the processing flow,\n",
    "but the model architcture is not realluy what I wanted. The main issue is the \n",
    "\"permutation invariant\" part, which was the GPT suggestion based on my request that \n",
    "the model be invariant with respect to the ordering of its inputs (i.e. the rows of the\n",
    "feature matrix). I am not convinced that this implementation really works.\n",
    "But either way I am abandoning this in favor of a different approach that is\n",
    "more verifiably permutation-invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d7995-33a4-44b6-a248-4d97c86ce57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca7e3-ead1-4ddd-b908-e3ecf2d2aef2",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee168238-1ed3-44c8-af9b-c1907aadfab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dimension of the (square) AOIs. Set thi to match what was used\n",
    "# when the tiles were created.\n",
    "aoi_size = 500\n",
    "\n",
    "# This is the resolution of the MPP encoding.\n",
    "resolution = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c1f54-443b-4820-ae36-b00ed6d575d1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216238e-a075-4a1d-81cf-3269d8c82fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of labels.\n",
    "fname = 'labels.csv'\n",
    "labels = pd.read_csv(fname)\n",
    "n_classes = len(labels)\n",
    "print('%d labels in this dataset' % n_classes)\n",
    "\n",
    "label_id_lookup = {\n",
    "    z['label']: z['id']\n",
    "    for z in labels.to_dict('records')\n",
    "}\n",
    "\n",
    "mask_label = label_id_lookup['token : mask']\n",
    "print('mask label: %s' % mask_label)\n",
    "\n",
    "pad_label = label_id_lookup['token : pad']\n",
    "print('pad label: %s' % pad_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ae19e-e8c4-41a4-8a34-9000d20698e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of input data files. Each file consists of a list of encodings for \n",
    "# a square tile.\n",
    "globstring = 'data/encodings/*'\n",
    "fnames = glob.glob(globstring)\n",
    "print('%d input files' % len(fnames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cdcfd-701e-44e5-9565-840775cf6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read some data.\n",
    "tile_data_list = []\n",
    "for fname in fnames[:5]:\n",
    "    print('reading', fname)\n",
    "    with open(fname, 'rb') as source:\n",
    "        tile_data_list += pickle.load(source)\n",
    "\n",
    "print('%d instances total' % len(tile_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ef620-9a45-4215-8fdb-b9b35f3820dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class wraps a list of input tile data as a pytorch dataset.\n",
    "# The \"getitem\" method here parses apart the true labels and the encodings,\n",
    "# and applies random masking to the endocing.\n",
    "\n",
    "class VergeDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_list, n_classes, mask_fraction=0.15, mask_label_index=-1):\n",
    "        self.data = data_list\n",
    "        self.n_classes = n_classes\n",
    "        self.mask_fraction = mask_fraction\n",
    "        self.encoding_dim = data_list[0].shape[1] - self.n_classes\n",
    "        self.mask_label_index = mask_label_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx]\n",
    "        encodings = features[:, self.n_classes:]\n",
    "        true_labels_onehot = features[:, :self.n_classes]\n",
    "        true_labels = np.argmax(true_labels_onehot, axis=1)\n",
    "        n_entities = features.shape[0]\n",
    "\n",
    "        # Define the \"mask\" label as a one-hot vector.\n",
    "        mask_label_onehot = np.zeros(self.n_classes)\n",
    "        mask_label_onehot[self.mask_label_index] = 1.0\n",
    "\n",
    "        # Select a few entities for which to assign masked labels.\n",
    "        mask = np.random.rand(n_entities) < self.mask_fraction\n",
    "        mask_indices = np.where(mask)\n",
    "\n",
    "        # Get true and masked labels as integer arrays.\n",
    "        masked_labels_onehot = copy.copy(true_labels_onehot)\n",
    "        for i in mask_indices:\n",
    "            masked_labels_onehot[i] = mask_label_onehot\n",
    "\n",
    "        # The features to be returned are a concatenation of the masked labels\n",
    "        # and the encodings\n",
    "        masked_features = torch.concat((\n",
    "            torch.tensor(masked_labels_onehot, dtype=torch.float32), \n",
    "            torch.tensor(encodings, dtype=torch.float32)\n",
    "        ), axis=-1)\n",
    "\n",
    "        # During model training below, we will be using the \"CrossEntropyLoss\" function,\n",
    "        # which has a built-in capability to ignore un-masked entitites. To get it to work,\n",
    "        # we need to pack the \"ignore\" token into any label slot that is not masked.\n",
    "        # Pytorch's standard value for that token is -100.\n",
    "        labels = torch.full(true_labels.shape, -100, dtype=torch.long)\n",
    "        for i in mask_indices[0]:\n",
    "            labels[i] = true_labels[i]\n",
    "        \n",
    "        return (masked_features, labels)\n",
    "\n",
    "dataset = VergeDataset(tile_data_list, n_classes, mask_label_index=mask_label)\n",
    "features, labels = dataset[0]\n",
    "print('features.shape', features.shape)\n",
    "print('labels.shape', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd86c9-551f-42f7-a303-c8f99af4c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New ChatGPT suggestion, using integer labels. This will need some editing.\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    max_len = max(x.shape[0] for x in features)\n",
    "    batch_size = len(features)\n",
    "    encoding_dim = features[0].shape[1]\n",
    "\n",
    "    padded_features = torch.zeros(batch_size, max_len, encoding_dim)\n",
    "    padded_labels = torch.full((batch_size, max_len), -100, dtype=torch.long)  # -100 is the \"ignore\" value\n",
    "    attention_mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        n = features[i].shape[0]\n",
    "        padded_features[i, :n] = features[i]\n",
    "        padded_labels[i, :n] = labels[i]\n",
    "        attention_mask[i, :n] = 1\n",
    "\n",
    "    return padded_features, padded_labels, attention_mask\n",
    "\n",
    "\n",
    "dataset = VergeDataset(tile_data_list, n_classes)\n",
    "batch = [dataset[k] for k in [5, 6, 7, 8]]\n",
    "batch_features, batch_labels, batch_attention_mask = collate_fn(batch)\n",
    "print('batch_features.shape', batch_features.shape)\n",
    "print('batch_labels.shape', batch_labels.shape)\n",
    "print('batch_attention_mask.shape', batch_attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b685c-980f-42d5-8d7a-a195b3f8705e",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075bfddc-5d69-4797-866d-a300b20fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GeospatialTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, model_dim=256, num_heads=4, num_layers=2, num_classes=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, model_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * model_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Allows [batch, seq, dim] format\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Optional permutation-invariant transformation\n",
    "        self.perm_invariant = nn.Sequential(\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.output_head = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape [batch_size, n_entities, encoding_dim]\n",
    "        attention_mask: Tensor of shape [batch_size, n_entities], with 1 for valid, 0 for padding\n",
    "        \"\"\"\n",
    "        x = self.input_proj(x)  # [batch, n_entities, model_dim]\n",
    "        # Transformer expects padding mask: True for PAD tokens\n",
    "        pad_mask = (attention_mask == 0)  # [batch, n_entities]\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)  # [batch, n_entities, model_dim]\n",
    "        x = self.perm_invariant(x)\n",
    "        logits = self.output_head(x)  # [batch, n_entities, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedea23-2671-49d1-85a8-ec5c557a8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeospatialTransformer(feature_dim=171, model_dim=64, num_heads=4, num_layers=2, num_classes=50, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116f8b4-2ad4-47ad-8c3d-7128e93456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = VergeDataset(tile_data_list, n_classes)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,            # Tune depending on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,   # Key for padding variable-length instances\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a432a9-dee4-42c8-a068-61d31492436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    for features, labels, attention_mask in dataloader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        logits = model(features, attention_mask)  \n",
    "        loss = criterion(\n",
    "            logits.view(-1, n_classes),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2246e-df72-48f6-98fa-8be691be1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.graph_objects import Scatter\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "trace = Scatter(\n",
    "    x=np.arange(len(losses)), y=losses, name='loss', \n",
    "    mode='markers+lines'\n",
    ")\n",
    "fig.append_trace(trace, 1, 1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b96949-62e9-4104-a3d7-1d45c66d5654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
