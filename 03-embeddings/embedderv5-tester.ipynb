{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2daeab-4b8b-411b-84cc-dd88286bf67b",
   "metadata": {},
   "source": [
    "# Test the embedding model\n",
    "\n",
    "The embedding odel code was generted by Claude.\n",
    "Here I'm testing it to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45908146-a047-477f-9f71-a417b5bc1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from embedderv5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7a034-8bc3-4b1b-8fa9-45f55d9e0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1fab08-3b50-4ae4-a9bc-c3b6e2812c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "sequences, similarity_features = generate_sample_data(\n",
    "    num_instances=1000, min_R=5, max_R=20, C=32, similarity_dim=16\n",
    ")\n",
    "for k, seq in enumerate(sequences):\n",
    "    print('sequence %d shape:' % k, sequences[k].shape)\n",
    "\n",
    "# print(len(similarity_features))\n",
    "# print(similarity_features[0].shape)\n",
    "# print(similarity_features[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479dc83-3452-4c86-a745-a6ba96a9df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader with explicit triplet sampling\n",
    "dataset = ContrastivePairDataset(\n",
    "    sequences, \n",
    "    similarity_features, \n",
    "    similarity_threshold=0.5,  # Adjust based on your similarity features\n",
    "    num_negatives=2  # Number of negatives per anchor\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=triplet_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2d22a-0ac0-4a11-a75d-b95811a398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PermutationInvariantModel(\n",
    "    input_dim=32,\n",
    "    hidden_dim=128,\n",
    "    embedding_dim=64,\n",
    "    num_attention_heads=4,  # Now using 4 attention heads\n",
    "    num_linear_layers=3,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb966d9b-f8f9-4067-84a1-09016d764c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs=20, learning_rate=1e-3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb371f-21f6-4296-af2d-135e7b48cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    # Unpack the dictionary structure from triplet data loader\n",
    "    anchor_seqs, anchor_masks, anchor_sims = sample_batch['anchor']\n",
    "    anchor_seqs = anchor_seqs.to(device)\n",
    "    anchor_masks = anchor_masks.to(device)\n",
    "    \n",
    "    # Generate embeddings for anchor samples\n",
    "    embeddings = model(anchor_seqs, anchor_masks)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Sample embedding norm: {torch.norm(embeddings[0]).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59d389-35e8-407f-888c-724ff2e59cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# Example inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    # Unpack the dictionary structure from triplet data loader\n",
    "    anchor_seqs, anchor_masks, anchor_sims = sample_batch['anchor']\n",
    "    anchor_seqs = anchor_seqs.to(device)\n",
    "    anchor_masks = anchor_masks.to(device)\n",
    "    \n",
    "    # Generate embeddings for anchor samples\n",
    "    embeddings = model(anchor_seqs, anchor_masks)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Sample embedding norm: {torch.norm(embeddings[0]).item():.4f}\")\n",
    "    \n",
    "    # Get embeddings for positives\n",
    "    pos_seqs, pos_masks, pos_sims = sample_batch['positive']\n",
    "    pos_seqs, pos_masks = pos_seqs.to(device), pos_masks.to(device)\n",
    "    pos_embeddings = model(pos_seqs, pos_masks)\n",
    "    print(f\"Positive embeddings shape: {pos_embeddings.shape}\")\n",
    "    \n",
    "    # Get embeddings for negatives\n",
    "    neg_seqs, neg_masks, neg_sims, neg_batch_indices = sample_batch['negatives']\n",
    "    neg_seqs, neg_masks = neg_seqs.to(device), neg_masks.to(device)\n",
    "    neg_batch_indices = neg_batch_indices.to(device)\n",
    "    neg_embeddings = model(neg_seqs, neg_masks)\n",
    "    print(f\"Negative embeddings shape: {neg_embeddings.shape}\")\n",
    "    \n",
    "    # Check similarity between anchors and positives\n",
    "    pos_similarities = F.cosine_similarity(embeddings, pos_embeddings, dim=1)\n",
    "    print(f\"Anchor-Positive similarities: {pos_similarities.mean().item():.4f} ± {pos_similarities.std().item():.4f}\")\n",
    "    \n",
    "    # Check similarity between anchors and negatives\n",
    "    batch_size = embeddings.shape[0]\n",
    "    neg_similarities_all = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get negatives for this anchor\n",
    "        neg_mask = neg_batch_indices == i\n",
    "        if neg_mask.sum() > 0:\n",
    "            anchor_i = embeddings[i:i+1]  # (1, embedding_dim)\n",
    "            negatives_i = neg_embeddings[neg_mask]  # (num_negs, embedding_dim)\n",
    "            \n",
    "            # Compute similarities between this anchor and its negatives\n",
    "            neg_sims_i = F.cosine_similarity(\n",
    "                anchor_i.expand_as(negatives_i), negatives_i, dim=1\n",
    "            )\n",
    "            neg_similarities_all.extend(neg_sims_i.cpu().tolist())\n",
    "    \n",
    "    if len(neg_similarities_all) > 0:\n",
    "        neg_similarities = torch.tensor(neg_similarities_all)\n",
    "        print(f\"Anchor-Negative similarities: {neg_similarities.mean().item():.4f} ± {neg_similarities.std().item():.4f}\")\n",
    "        \n",
    "        # Show the difference (should be positive if model is learning well)\n",
    "        print(f\"Positive vs Negative similarity difference: {pos_similarities.mean().item() - neg_similarities.mean().item():.4f}\")\n",
    "    else:\n",
    "        print(\"No negative samples found in this batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba3498-99f8-4565-b0fb-8208f9b8b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    # Unpack the dictionary structure from triplet data loader\n",
    "    anchor_seqs, anchor_masks, anchor_sims = sample_batch['anchor']\n",
    "    anchor_seqs = anchor_seqs.to(device)\n",
    "    anchor_masks = anchor_masks.to(device)\n",
    "    \n",
    "    # Generate embeddings for anchor samples\n",
    "    embeddings = model(anchor_seqs, anchor_masks)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Sample embedding norm: {torch.norm(embeddings[0]).item():.4f}\")\n",
    "    \n",
    "    # You can also get embeddings for positives and negatives\n",
    "    pos_seqs, pos_masks, pos_sims = sample_batch['positive']\n",
    "    pos_seqs, pos_masks = pos_seqs.to(device), pos_masks.to(device)\n",
    "    pos_embeddings = model(pos_seqs, pos_masks)\n",
    "    print(f\"Positive embeddings shape: {pos_embeddings.shape}\")\n",
    "    \n",
    "    # Check similarity between anchors and positives\n",
    "    similarities = F.cosine_similarity(embeddings, pos_embeddings, dim=1)\n",
    "    print(f\"Anchor-Positive similarities: {similarities.mean().item():.4f} ± {similarities.std().item():.4f}\")\n",
    "\n",
    "    # You can also get embeddings for negatives\n",
    "    neg_seqs, neg_masks, neg_sims = sample_batch['negatives']\n",
    "    neg_seqs, neg_masks = negs_seqs.to(device), neg_masks.to(device)\n",
    "    neg_embeddings = model(neg_seqs, neg_masks)\n",
    "    print(f\"Positive embeddings shape: {neg_embeddings.shape}\")\n",
    "    \n",
    "    # Check similarity between anchors and negatives\n",
    "    similarities = F.cosine_similarity(embeddings, neg_embeddings, dim=1)\n",
    "    print(f\"Anchor-Negative similarities: {similarities.mean().item():.4f} ± {similarities.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce65fc9-f3c1-481e-a1a3-eaac1025316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sample_batch['positive']\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0502e3-8407-4184-b08d-ceb03851fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0]\n",
    "b.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
