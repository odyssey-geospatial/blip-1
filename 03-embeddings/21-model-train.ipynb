{"cells":[{"cell_type":"markdown","id":"3e2daeab-4b8b-411b-84cc-dd88286bf67b","metadata":{"id":"3e2daeab-4b8b-411b-84cc-dd88286bf67b"},"source":["# Train the embedding model"]},{"cell_type":"markdown","id":"9fdc7f7f-2399-4755-b7db-052b0132479a","metadata":{"id":"9fdc7f7f-2399-4755-b7db-052b0132479a"},"source":["## Processing Setup"]},{"cell_type":"code","execution_count":null,"id":"5856a24c-41a8-45fe-b254-8e4c31b330d9","metadata":{"id":"5856a24c-41a8-45fe-b254-8e4c31b330d9"},"outputs":[],"source":["# Google colab\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","project_home = '/content/drive/MyDrive/Projects/verge'\n","os.chdir(project_home)"]},{"cell_type":"code","execution_count":null,"id":"96eefe45-43a4-4665-8b29-484e2b32ae66","metadata":{"id":"96eefe45-43a4-4665-8b29-484e2b32ae66"},"outputs":[],"source":["# Local processing setup\n","# project_home = '..'"]},{"cell_type":"markdown","id":"928178a0-d593-4266-a683-5f9c5e3830ba","metadata":{"id":"928178a0-d593-4266-a683-5f9c5e3830ba"},"source":["## Notebook Setup"]},{"cell_type":"code","execution_count":null,"id":"45908146-a047-477f-9f71-a417b5bc1c0e","metadata":{"id":"45908146-a047-477f-9f71-a417b5bc1c0e"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","from typing import List, Tuple, Optional\n","\n","import pickle\n","import pandas as pd\n","\n","import sys\n","sys.path.append('%s/03-embeddings' % project_home)\n","from embedderv5 import *"]},{"cell_type":"markdown","id":"48aa408e-db4c-4f59-8a22-993ee4ec5a85","metadata":{"id":"48aa408e-db4c-4f59-8a22-993ee4ec5a85"},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"id":"6886ae2e-4c09-4a9d-8104-5fd506c75ad8","metadata":{"id":"6886ae2e-4c09-4a9d-8104-5fd506c75ad8"},"outputs":[],"source":["# The name of the ROI to use.\n","roi_name = 'newengland'\n","\n","# The name of the general-purpose data directory.\n","data_home = '%s/data' % (project_home)\n","\n","# The name of the ROI-specific data directory.\n","roi_home = '%s/data/%s' % (project_home, roi_name)\n","\n","# The unique identifier of the model to be used.\n","run_id = '201b'\n","\n","# Identifier of the splits file.\n","splits_id = '201'\n","\n","# What type of device to train on.\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('using device', device)\n","\n","# Number of epochs for whch to train the model.\n","num_epochs = 10\n"]},{"cell_type":"markdown","id":"13ffc010-4ee2-43ad-b82f-0bfb063c0de7","metadata":{"id":"13ffc010-4ee2-43ad-b82f-0bfb063c0de7"},"source":["## Load and organize data\n","We have two data sources that we need to associate with one another:\n","a set of initial embeddings (\"initials\") and a set of feature vectors\n","to be used for similarity assessments (\"features\")."]},{"cell_type":"code","execution_count":null,"id":"b377b4b0-8101-4ac8-88b9-547570f3b741","metadata":{"id":"b377b4b0-8101-4ac8-88b9-547570f3b741"},"outputs":[],"source":["# We will divide into training and validation sets based on AOI.\n","# The splits have already been determined, before training the initial MGM.\n","# Here we look them up and re-organize things a bit.\n","fname = '%s/models/splits-%s.csv' % (roi_home, splits_id)\n","splits = pd.read_csv(fname)\n","print('%d splits' % len(splits))\n","splits.head(3)\n","\n","splits_lookup = {\n","    '%s : %s' % (z['aoi_tag'], z['tile_tag']): z['split']\n","    for k, z in splits.iterrows()\n","}\n","print('%d elements in splits lookup' % len(splits_lookup))\n"]},{"cell_type":"code","execution_count":null,"id":"2a047a46-74d7-426a-ac1a-502f8770eda6","metadata":{"id":"2a047a46-74d7-426a-ac1a-502f8770eda6"},"outputs":[],"source":["# Get a list of tiles.\n","fname = '%s/tiles.csv' % roi_home\n","tile_info = pd.read_csv(fname)\n","print('%d tiles' % len(tile_info))\n","tile_info.head(3)"]},{"cell_type":"code","execution_count":null,"id":"038c2578-f48e-4b78-922a-e6e2829a7fb5","metadata":{"id":"038c2578-f48e-4b78-922a-e6e2829a7fb5"},"outputs":[],"source":["# Get the list of AOI tags.\n","aoi_tags = np.unique(tile_info['aoi_tag'])\n","print('%d unique AOIs' % len(aoi_tags))"]},{"cell_type":"code","execution_count":null,"id":"2c0c2b64-203f-42c6-b5b8-8d15ea3cca7d","metadata":{"id":"2c0c2b64-203f-42c6-b5b8-8d15ea3cca7d"},"outputs":[],"source":["# Load initial embeddings. Put them into a lookup table based on aoi/tile identifiers.\n","embeddings_lookup = {}\n","\n","for k, aoi_tag in enumerate(aoi_tags):\n","\n","    if k % 20 == 0:\n","      print('%d/%d' % (k, len(aoi_tags)))\n","\n","    fname = '%s/initials/%s.pkl' % (roi_home, aoi_tag)\n","    with open(fname, 'rb') as source:\n","        a = pickle.load(source)\n","    for b in a:\n","        key = '%s : %s' % (b['aoi_tag'], b['tile_tag'])\n","        e = b['embedding']\n","        embeddings_lookup[key] = e\n","\n","print('%d total embeddings' % len(embeddings_lookup))\n"]},{"cell_type":"code","execution_count":null,"id":"a9c50cfe-f126-45e8-ad9f-ead1406c303a","metadata":{"id":"a9c50cfe-f126-45e8-ad9f-ead1406c303a"},"outputs":[],"source":["embedding_dim = e.shape[-1]\n","print('dimension of embeddings is %d' % embedding_dim)"]},{"cell_type":"code","execution_count":null,"id":"199d516d-0071-4acd-9be5-363d10e38d69","metadata":{"id":"199d516d-0071-4acd-9be5-363d10e38d69"},"outputs":[],"source":["# Load initial Features. Ditto.\n","features_lookup = {}\n","\n","for k, aoi_tag in enumerate(aoi_tags):\n","\n","    if k % 20 == 0:\n","      print('%d/%d' % (k, len(aoi_tags)))\n","\n","    fname = '%s/features/%s.pkl' % (roi_home, aoi_tag)\n","    with open(fname, 'rb') as source:\n","        a = pickle.load(source)\n","    for b in a:\n","        key = '%s : %s' % (b['aoi_tag'], b['tile_tag'])\n","        f = b['features']\n","        features_lookup[key] = f\n","\n","print('%d total feature vectors' % len(features_lookup))\n"]},{"cell_type":"code","source":["# Organize the data the way the model expects it.\n","train_sequences = []\n","train_features = []\n","val_sequences = []\n","val_features = []\n","for key in features_lookup.keys():\n","    f = features_lookup[key]\n","    if key in embeddings_lookup:\n","        e = embeddings_lookup[key].squeeze().detach().cpu().numpy()\n","        if splits_lookup[key] == 'train':\n","            train_sequences.append(e)\n","            train_features.append(f)\n","        elif splits_lookup[key] == 'val':\n","            val_sequences.append(e)\n","            val_features.append(f)\n","    else:\n","        print('key mismatch: %s' % key)\n","\n","print('%d training sequences' % len(train_sequences))\n","print('%d val sequences' % len(val_sequences))\n","\n","#\n"],"metadata":{"id":"kLOxT-yyOB--"},"id":"kLOxT-yyOB--","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0938625d-a7b7-47ac-807f-b726d3bae9cb","metadata":{"id":"0938625d-a7b7-47ac-807f-b726d3bae9cb"},"outputs":[],"source":["# # Organize the data the way the model expects it.\n","# sequences = []\n","# similarity_features = []\n","# for key in features_lookup.keys():\n","#     f = features_lookup[key]\n","#     if key in embeddings_lookup:\n","#         e = embeddings_lookup[key].squeeze().detach().cpu().numpy()\n","#         sequences.append(e)\n","#         similarity_features.append(f)\n","#     else:\n","#         print('key mismatch: %s' % key)\n","\n","# print(len(sequences))\n","# print(type(sequences))\n","# print(type(sequences[0]))\n","# print(sequences[0].shape)\n"]},{"cell_type":"code","execution_count":null,"id":"3b1179dc-ea51-4da9-bafc-c457f6004b82","metadata":{"id":"3b1179dc-ea51-4da9-bafc-c457f6004b82"},"outputs":[],"source":["# # # This was the Claude-generated code to generate test data for the model.\n","# sequences, similarity_features = generate_sample_data(\n","#     num_instances=1000, min_R=5, max_R=20, C=32, similarity_dim=16\n","# )\n","# print(type(sequences))\n","# print(type(sequences[0]))\n","# print(sequences[0].shape)"]},{"cell_type":"markdown","id":"fd5c18a8-ba3c-4a08-82ae-c2bdfc079075","metadata":{"id":"fd5c18a8-ba3c-4a08-82ae-c2bdfc079075"},"source":["## Model\n","The model code, including data loaders, the model itself, loss function, and all that,\n","were generated by Claude via a lot of iterative prompting and debugging.\n"]},{"cell_type":"code","execution_count":null,"id":"3479dc83-3452-4c86-a745-a6ba96a9df68","metadata":{"id":"3479dc83-3452-4c86-a745-a6ba96a9df68"},"outputs":[],"source":["# Create dataset and dataloader with explicit triplet sampling\n","train_dataset = ContrastivePairDataset(\n","    train_sequences,\n","    train_features,\n","    similarity_threshold=0.5,  # Adjust based on your similarity features\n","    num_negatives=2  # Number of negatives per anchor\n",")\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=triplet_collate_fn)\n","\n","val_dataset = ContrastivePairDataset(\n","    val_sequences,\n","    val_features,\n","    similarity_threshold=0.5,  # Adjust based on your similarity features\n","    num_negatives=2  # Number of negatives per anchor\n",")\n","val_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=triplet_collate_fn)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6cc2d22a-0ac0-4a11-a75d-b95811a398d1","metadata":{"id":"6cc2d22a-0ac0-4a11-a75d-b95811a398d1"},"outputs":[],"source":["# Initialize model\n","model = PermutationInvariantModel(\n","    input_dim=embedding_dim,\n","    hidden_dim=128,\n","    embedding_dim=embedding_dim,\n","    num_attention_heads=8,\n","    num_linear_layers=3,\n","    dropout=0.1\n",")\n","\n","print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n"]},{"cell_type":"code","source":["def train_model_with_triplets(\n","    model, train_loader, val_loader,\n","    num_epochs=100, learning_rate=1e-3,\n","    device='cpu'\n","):\n","    \"\"\"\n","    Training loop for the permutation-invariant model with explicit triplet sampling.\n","    \"\"\"\n","\n","    train_loss_history = []\n","    val_loss_history = []\n","    min_avg_val_loss = None\n","\n","    model = model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","    criterion = TripletContrastiveLoss(margin=0.5, temperature=0.1)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        total_loss = 0.0\n","        num_batches = 0\n","\n","        for batch_idx, batch_data in enumerate(train_loader):\n","            # Unpack batch data\n","            anchor_seqs, anchor_masks, anchor_sims = batch_data['anchor']\n","            pos_seqs, pos_masks, pos_sims = batch_data['positive']\n","            neg_seqs, neg_masks, neg_sims, neg_batch_indices = batch_data['negatives']\n","\n","            # Move to device\n","            anchor_seqs, anchor_masks = anchor_seqs.to(device), anchor_masks.to(device)\n","            pos_seqs, pos_masks = pos_seqs.to(device), pos_masks.to(device)\n","            neg_seqs, neg_masks = neg_seqs.to(device), neg_masks.to(device)\n","            neg_batch_indices = neg_batch_indices.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            anchor_emb = model(anchor_seqs, anchor_masks)\n","            pos_emb = model(pos_seqs, pos_masks)\n","            neg_emb = model(neg_seqs, neg_masks)\n","\n","            # Compute loss\n","            loss = criterion(anchor_emb, pos_emb, neg_emb, neg_batch_indices)\n","\n","            # Backward pass\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            num_batches += 1\n","\n","\n","        # Proces the validation dataset.\n","        total_val_loss = 0.0\n","        num_val_batches = 0\n","\n","        model.eval()\n","        with torch.no_grad():\n","          for batch_idx, batch_data in enumerate(val_loader):\n","              # Unpack batch data\n","              anchor_seqs, anchor_masks, anchor_sims = batch_data['anchor']\n","              pos_seqs, pos_masks, pos_sims = batch_data['positive']\n","              neg_seqs, neg_masks, neg_sims, neg_batch_indices = batch_data['negatives']\n","\n","              # Move to device\n","              anchor_seqs, anchor_masks = anchor_seqs.to(device), anchor_masks.to(device)\n","              pos_seqs, pos_masks = pos_seqs.to(device), pos_masks.to(device)\n","              neg_seqs, neg_masks = neg_seqs.to(device), neg_masks.to(device)\n","              neg_batch_indices = neg_batch_indices.to(device)\n","\n","              # Forward pass\n","              anchor_emb = model(anchor_seqs, anchor_masks)\n","              pos_emb = model(pos_seqs, pos_masks)\n","              neg_emb = model(neg_seqs, neg_masks)\n","\n","              # Compute loss\n","              val_loss = criterion(anchor_emb, pos_emb, neg_emb, neg_batch_indices)\n","              total_val_loss += val_loss.item()\n","              num_val_batches += 1\n","\n","        scheduler.step()\n","\n","        avg_loss = total_loss / num_batches\n","        avg_val_loss = total_val_loss / num_val_batches\n","        print(f'Epoch {epoch} - Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n","\n","        train_loss_history.append(avg_loss)\n","        val_loss_history.append(avg_val_loss)\n","\n","        if min_avg_val_loss is None or avg_val_loss < min_avg_val_loss:\n","            min_avg_val_loss = avg_val_loss\n","            model_fname = '%s/models/model-%s.pth' % (roi_home, run_id)\n","            print('==> Saving model to %s' % model_fname)\n","            torch.save(model.state_dict(), model_fname)\n","\n","    return train_loss_history, val_loss_history\n"],"metadata":{"id":"ZdkMwcA9QmUH"},"id":"ZdkMwcA9QmUH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"eb966d9b-f8f9-4067-84a1-09016d764c24","metadata":{"id":"eb966d9b-f8f9-4067-84a1-09016d764c24"},"outputs":[],"source":["# Train the model\n","train_loss_history, val_loss_history = train_model_with_triplets(\n","    model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=1e-3,\n","    device=device\n",")\n"]},{"cell_type":"code","source":["# Plot the trainign and validation loss history using plotly.\n","import plotly\n","from plotly.graph_objects import Scatter\n","from plotly.subplots import make_subplots\n","\n","fig = make_subplots(rows=1, cols=1)\n","\n","train_loss_history = np.array(train_loss_history)\n","val_loss_history = np.array(val_loss_history)\n","\n","fig.add_trace(\n","    Scatter(y=train_loss_history, name='Training Loss', mode='markers+lines'),\n","    row=1, col=1\n",")\n","\n","fig.add_trace(\n","    Scatter(y=val_loss_history, name='Validation Loss', mode='markers+lines'),\n","    row=1, col=1\n",")\n","\n","fig.update_layout(\n","    title='Training and Validation Loss',\n","    xaxis_title='Epoch',\n","    yaxis_title='Loss',\n","    height=400,\n","    width=900,\n",")\n","\n","fig.show()\n"],"metadata":{"id":"kQnY1wara7RN"},"id":"kQnY1wara7RN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8e59d389-35e8-407f-888c-724ff2e59cf2","metadata":{"id":"8e59d389-35e8-407f-888c-724ff2e59cf2"},"outputs":[],"source":["\n","# Example inference\n","model.eval()\n","with torch.no_grad():\n","    sample_batch = next(iter(train_loader))\n","    # Unpack the dictionary structure from triplet data loader\n","    anchor_seqs, anchor_masks, anchor_sims = sample_batch['anchor']\n","    anchor_seqs = anchor_seqs.to(device)\n","    anchor_masks = anchor_masks.to(device)\n","\n","    # Generate embeddings for anchor samples\n","    embeddings = model(anchor_seqs, anchor_masks)\n","    print(f\"Generated embeddings shape: {embeddings.shape}\")\n","    print(f\"Sample embedding norm: {torch.norm(embeddings[0]).item():.4f}\")\n","\n","    # Get embeddings for positives\n","    pos_seqs, pos_masks, pos_sims = sample_batch['positive']\n","    pos_seqs, pos_masks = pos_seqs.to(device), pos_masks.to(device)\n","    pos_embeddings = model(pos_seqs, pos_masks)\n","    print(f\"Positive embeddings shape: {pos_embeddings.shape}\")\n","\n","    # Get embeddings for negatives\n","    neg_seqs, neg_masks, neg_sims, neg_batch_indices = sample_batch['negatives']\n","    neg_seqs, neg_masks = neg_seqs.to(device), neg_masks.to(device)\n","    neg_batch_indices = neg_batch_indices.to(device)\n","    neg_embeddings = model(neg_seqs, neg_masks)\n","    print(f\"Negative embeddings shape: {neg_embeddings.shape}\")\n","\n","    # Check similarity between anchors and positives\n","    pos_similarities = F.cosine_similarity(embeddings, pos_embeddings, dim=1)\n","    print(f\"Anchor-Positive similarities: {pos_similarities.mean().item():.4f} ± {pos_similarities.std().item():.4f}\")\n","\n","    # Check similarity between anchors and negatives\n","    batch_size = embeddings.shape[0]\n","    neg_similarities_all = []\n","\n","    for i in range(batch_size):\n","        # Get negatives for this anchor\n","        neg_mask = neg_batch_indices == i\n","        if neg_mask.sum() > 0:\n","            anchor_i = embeddings[i:i+1]  # (1, embedding_dim)\n","            negatives_i = neg_embeddings[neg_mask]  # (num_negs, embedding_dim)\n","\n","            # Compute similarities between this anchor and its negatives\n","            neg_sims_i = F.cosine_similarity(\n","                anchor_i.expand_as(negatives_i), negatives_i, dim=1\n","            )\n","            neg_similarities_all.extend(neg_sims_i.cpu().tolist())\n","\n","    if len(neg_similarities_all) > 0:\n","        neg_similarities = torch.tensor(neg_similarities_all)\n","        print(f\"Anchor-Negative similarities: {neg_similarities.mean().item():.4f} ± {neg_similarities.std().item():.4f}\")\n","\n","        # Show the difference (should be positive if model is learning well)\n","        print(f\"Positive vs Negative similarity difference: {pos_similarities.mean().item() - neg_similarities.mean().item():.4f}\")\n","    else:\n","        print(\"No negative samples found in this batch\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Rlnzl2XXEfHi"},"id":"Rlnzl2XXEfHi","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}